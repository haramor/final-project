# Women's Health Contraceptive Research Assistant (XX)

A personalized, AI-powered tool for accessing medically relevant research on contraception using a combination of local Large Language Models (LLMs) via Ollama and Retrieval-Augmented Generation (RAG) technology.

## Project Overview

This application empowers users to query medical information about contraception using natural language. The system retrieves relevant information from a curated set of research paper abstracts and generates synthesized, evidence-based responses, taking into account the user's provided profile information (current contraceptive methods, experienced side effects, and age group).

Key Features:
- Natural language interface for contraceptive queries.
- Personalized insights based on user-provided profile information.
- Evidence-based responses generated by LLMs (e.g., Llama 3 via Ollama) using information from medical literature.
- In-line citation of source documents for credibility.
- Clear listing of all retrieved source titles.
- Filtering options in the UI for user context (though filtering of retrieved documents based on these is a potential future enhancement).

## Project Structure

```
.
├── backend/                 # Python Flask backend
│   ├── app/
│   │   ├── database/       # ChromaDB vector database setup, data loading (db.py, config.py)
│   │   ├── rag/           # RAG implementation (rag_service.py: prompt, LLM interaction)
│   │   ├── routes/        # API endpoints (rag.py, pubmed.py)
│   │   └── __init__.py    # Flask app creation
│   ├── venv/               # Python virtual environment
│   ├── run.py              # Script to run the Flask development server
│   └── requirements.txt    # Python dependencies
│
├── frontend/               # React Vite frontend
│   ├── public/
│   ├── src/
│   │   ├── components/    # React components (e.g., SearchForm.jsx)
│   │   └── services/      # API integration (api.js)
│   ├── .env.example        # Example for frontend environment variables
│   └── package.json       # Node dependencies
│
├── data/
│   └── chroma_db/          # Default persistence directory for ChromaDB
│
├── pubmed_contraception_abstracts2.json # Example data file for RAG
└── render.yaml             # Deployment configuration for Render
└── README.md               # This file
```

## How the RAG System Works

1.  **User Input**: The user asks a question and optionally provides their current contraceptive method, side effects they are experiencing, and age group through the frontend UI.
2.  **API Request**: The frontend sends this information to the backend `/api/rag_query` endpoint.
3.  **Document Retrieval**:
    *   The backend's RAG service (`rag_service.py`) uses a Langchain retriever.
    *   This retriever queries a ChromaDB vector database (stored locally in `backend/data/chroma_db/` by default).
    *   The database contains embeddings of research paper abstracts (e.g., from `pubmed_contraception_abstracts2.json`).
    *   The retriever fetches the top K (currently 5) document chunks most semantically similar to the user's question.
4.  **Prompt Engineering**:
    *   The retrieved document chunks (with their source titles), the user's original question, and their profile data are dynamically inserted into a carefully designed prompt (`RAG_PROMPT_TEMPLATE` in `rag_service.py`).
    *   This prompt instructs the LLM to act as a caring and empathetic women's health information assistant, to synthesize information relevant to the question and user profile, to cite sources in-line, and to suggest personalized follow-up questions for healthcare discussions.
5.  **LLM Interaction**:
    *   The complete prompt is sent to a Large Language Model (LLM) running locally via Ollama (e.g., Llama 3, as configured in `rag_service.py`).
    *   Ollama must be running (`ollama serve`) and have the specified model downloaded (`ollama pull llama3`).
6.  **Response Generation**: The LLM generates a textual answer based on the provided context and instructions.
7.  **Source Handling**:
    *   The LLM is instructed to mention source titles in-line when citing specific information.
    *   The backend also separately extracts a list of all unique titles from the K retrieved documents.
8.  **Frontend Display**: The frontend displays the LLM's generated answer and the separate list of source document titles.
9.  **Dropdown Options**: A separate `/dropdown-options` endpoint provides hardcoded choices for the user profile filters in the UI.

## Getting Started

### Prerequisites
- Python (version as specified in `backend/runtime.txt`, or 3.8+)
- Node.js (16+ or as per `frontend/package.json` engines)
- Ollama: Install from [ollama.com](https://ollama.com/).

### Backend Setup

1.  **Navigate to the backend directory:**
    ```bash
    cd xx/backend
    ```

2.  **Create and activate a Python virtual environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Set up Ollama:**
    *   Ensure Ollama is installed.
    *   In a **separate terminal**, start the Ollama server:
        ```bash
        ollama serve
        ```
        (Keep this terminal running. If you have the Ollama Desktop app, it might already be running the server).
    *   In **another separate terminal**, pull the required LLM model (default is Llama 3):
        ```bash
        ollama pull llama3
        ```

5.  **Populate the Vector Database (ChromaDB):**
    *   The RAG system needs data to retrieve. An example data file is `xx/pubmed_contraception_abstracts2.json`.
    *   The script `xx/backend/app/database/db.py` is responsible for loading this data into ChromaDB.
    *   **Important**: Open `xx/backend/app/database/db.py`. Near the end, in the `if __name__ == "__main__":` block, ensure the path in `db.preprocess_and_add_json("PATH_TO_YOUR_DATA.json", "research_papers")` correctly points to your data file (e.g., `"../../../pubmed_contraception_abstracts2.json"` if `pubmed_contraception_abstracts2.json` is in the `xx/` directory).
    *   From the `xx/backend/app/database/` directory (while your venv is active), run:
        ```bash
        python db.py
        ```
        This will process the JSON file and populate the ChromaDB store (default location: `xx/data/chroma_db/`). This step only needs to be done once, or when your source data changes.

6.  **Environment Variables (Optional but Good Practice):**
    *   The application can use a `.env` file in the `xx/backend/` directory for certain configurations (e.g., if you were to integrate other API keys). Currently, most critical settings like model names are hardcoded or have defaults in the Python files (e.g. `rag_service.py`, `database/config.py`).

7.  **Run the backend server:**
    *   Ensure you are in the `xx/backend/` directory and your virtual environment is active.
    *   Run:
        ```bash
        python run.py
        ```
    *   The backend server will typically start on `http://localhost:5001`.

### Frontend Setup

1.  **Navigate to the frontend directory:**
    ```bash
    cd xx/frontend
    ```

2.  **Install Node.js dependencies:**
    ```bash
    npm install
    ```

3.  **Environment Variables:**
    *   The frontend needs to know where the backend API is running. This is configured via `VITE_API_URL`.
    *   You can create a `.env` file in the `xx/frontend/` directory (copy from `.env.example` if one exists, or create new).
    *   Add the following line to your `xx/frontend/.env` file:
        ```
        VITE_API_URL=http://localhost:5001
        ```
    *   If you don't use a `.env` file, the default is hardcoded in `src/services/api.js`, ensure it's correct (e.g., `http://localhost:5001`).

4.  **Start the frontend development server:**
    ```bash
    npm run dev
    ```
    *   The frontend will typically start on `http://localhost:5173`. Open this URL in your browser.

## Development Status

### Completed
- Basic PubMed search functionality for contraceptive research
- Document processing pipeline for medical papers
- Database schema and models
- Frontend search interface
- Basic RAG implementation for contraceptive queries
- Integration with local LLM (Llama 3 via Ollama)
- Personalized responses based on user profile
- ChromaDB for vector storage

### In Progress
- Enhanced RAG capabilities for more accurate and nuanced responses (ongoing prompt engineering)
- Improved document processing for better information extraction
- User experience improvements for better accessibility

### Planned
- Advanced filtering of retrieved documents based on user profile for even more targeted context.
- Fine-tuning model on medical data using LORA or other technique (if required).
- More robust error handling and user feedback.

## Example Queries

The system can handle questions like:
- "What are the side effects of an IUD?" (Try selecting "IUD" in your profile)
- "How effective are hormonal patches if I'm experiencing mood swings?"
- "What contraception methods are best for women over 40?"
- "What are the risks of long-term birth control use considering my age?"

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

